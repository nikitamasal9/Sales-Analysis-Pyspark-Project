{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d22b90-235b-45b0-9ea5-93195e746925",
   "metadata": {},
   "source": [
    "Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6380ca9-ff57-4497-81c8-0f5c4adb66fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304a4dd6-b879-4e74-9f83-a21c9a34ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78651f83-68c2-4e97-ae76-b0b6e5e9d137",
   "metadata": {},
   "source": [
    "A spark session has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9b63c3c-9f05-4234-a765-580916684314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/10 14:26:31 WARN Utils: Your hostname, DESKTOP-77VPNBL resolves to a loopback address: 127.0.1.1; using 172.25.173.185 instead (on interface eth0)\n",
      "24/03/10 14:26:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/10 14:26:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Spark project')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e093d1-11b7-404f-96a4-9f7bab2ea119",
   "metadata": {},
   "source": [
    "Importing necessary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20dc1cf-5aec-4614-96e1-3409b37e933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"source_order\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619cb92d-dc19-4eb4-bf4a-19e4a3cfc840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[product_id: int, customer_id: string, order_date: date, location: string, source_order: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sales_df = spark.read.format(\"csv\").option(\"inferschema\",\"true\").schema(schema).load(\"../dataset/sales.csv.txt\")\n",
    "display(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "489670ed-e33c-4c5a-ba2f-e2b668c34af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+--------+------------+\n",
      "|product_id|customer_id|order_date|location|source_order|\n",
      "+----------+-----------+----------+--------+------------+\n",
      "|         1|          A|2023-01-01|   India|      Swiggy|\n",
      "|         2|          A|2022-01-01|   India|      Swiggy|\n",
      "|         2|          A|2023-01-07|   India|      Swiggy|\n",
      "|         3|          A|2023-01-10|   India|  Restaurant|\n",
      "|         3|          A|2022-01-11|   India|      Swiggy|\n",
      "|         3|          A|2023-01-11|   India|  Restaurant|\n",
      "|         2|          B|2022-02-01|   India|      Swiggy|\n",
      "|         2|          B|2023-01-02|   India|      Swiggy|\n",
      "|         1|          B|2023-01-04|   India|  Restaurant|\n",
      "|         1|          B|2023-02-11|   India|      Swiggy|\n",
      "|         3|          B|2023-01-16|   India|      zomato|\n",
      "|         3|          B|2022-02-01|   India|      zomato|\n",
      "|         3|          C|2023-01-01|   India|      zomato|\n",
      "|         1|          C|2023-01-01|      UK|      Swiggy|\n",
      "|         6|          C|2022-01-07|      UK|      zomato|\n",
      "|         3|          D|2023-02-16|      UK|  Restaurant|\n",
      "|         5|          D|2022-02-01|      UK|      zomato|\n",
      "|         3|          E|2023-02-01|      UK|  Restaurant|\n",
      "|         4|          E|2023-02-01|      UK|      Swiggy|\n",
      "|         4|          E|2023-02-07|      UK|  Restaurant|\n",
      "+----------+-----------+----------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5113a8e4-1dea-4893-a5e9-cb8e6e7c22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"price\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8174b45a-ec51-4121-92b1-ff7e739b5bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[product_id: int, customer_id: string, order_date: date, location: string, source_order: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "menu_df = spark.read.format(\"csv\").option(\"inferschema\",\"true\").schema(schema).load(\"../dataset/menu.csv.txt\")\n",
    "display(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98906d1c-a74a-4455-848b-a7aaf3484a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+\n",
      "|product_id|product_name|price|\n",
      "+----------+------------+-----+\n",
      "|         1|       PIZZA| null|\n",
      "|         2|     Chowmin| null|\n",
      "|         3|    sandwich| null|\n",
      "|         4|        Dosa| null|\n",
      "|         5|     Biryani| null|\n",
      "|         6|       Pasta| null|\n",
      "+----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "menu_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d7a920-f794-42ee-bcc3-51473ca8267c",
   "metadata": {},
   "source": [
    "Deriving year, month and quarter from the given dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44e23734-3f61-4e67-82d2-66ace686ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, quarter, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7ab797f-768c-4daa-8b03-a359fd76537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df.withColumn('order_year', year(sales_df['order_date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6cdde4e-3f87-44cc-9d8b-7c44c2b5acc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df.withColumn('order_quarter', quarter(sales_df['order_date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a20806-1f86-4773-ac27-418aed3b145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = sales_df.withColumn('order_month', month(sales_df['order_date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11588a2d-7989-45a8-b378-548535092498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+--------+------------+----------+-------------+-----------+\n",
      "|product_id|customer_id|order_date|location|source_order|order_year|order_quarter|order_month|\n",
      "+----------+-----------+----------+--------+------------+----------+-------------+-----------+\n",
      "|         1|          A|2023-01-01|   India|      Swiggy|      2023|            1|          1|\n",
      "|         2|          A|2022-01-01|   India|      Swiggy|      2022|            1|          1|\n",
      "|         2|          A|2023-01-07|   India|      Swiggy|      2023|            1|          1|\n",
      "|         3|          A|2023-01-10|   India|  Restaurant|      2023|            1|          1|\n",
      "|         3|          A|2022-01-11|   India|      Swiggy|      2022|            1|          1|\n",
      "|         3|          A|2023-01-11|   India|  Restaurant|      2023|            1|          1|\n",
      "|         2|          B|2022-02-01|   India|      Swiggy|      2022|            1|          2|\n",
      "|         2|          B|2023-01-02|   India|      Swiggy|      2023|            1|          1|\n",
      "|         1|          B|2023-01-04|   India|  Restaurant|      2023|            1|          1|\n",
      "|         1|          B|2023-02-11|   India|      Swiggy|      2023|            1|          2|\n",
      "|         3|          B|2023-01-16|   India|      zomato|      2023|            1|          1|\n",
      "|         3|          B|2022-02-01|   India|      zomato|      2022|            1|          2|\n",
      "|         3|          C|2023-01-01|   India|      zomato|      2023|            1|          1|\n",
      "|         1|          C|2023-01-01|      UK|      Swiggy|      2023|            1|          1|\n",
      "|         6|          C|2022-01-07|      UK|      zomato|      2022|            1|          1|\n",
      "|         3|          D|2023-02-16|      UK|  Restaurant|      2023|            1|          2|\n",
      "|         5|          D|2022-02-01|      UK|      zomato|      2022|            1|          2|\n",
      "|         3|          E|2023-02-01|      UK|  Restaurant|      2023|            1|          2|\n",
      "|         4|          E|2023-02-01|      UK|      Swiggy|      2023|            1|          2|\n",
      "|         4|          E|2023-02-07|      UK|  Restaurant|      2023|            1|          2|\n",
      "+----------+-----------+----------+--------+------------+----------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4516ecee-138e-4608-a248-65604b90469c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
